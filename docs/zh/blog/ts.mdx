---
title: '构建开源AI硬件评测工具， ByteMLperf-v1.0 X 天数智芯，扬长避短是关键'
date: 2025-11-21
weight: 2
keywords: ['ByteMLPerf', '天数智芯']
---

import image1 from '../../public/blog/ts/image1.png';
import image2 from '../../public/blog/ts/image2.png';
import image3 from '../../public/blog/ts/image3.png';
import image4 from '../../public/blog/ts/image4.png';
import image5 from '../../public/blog/ts/image5.png';
import image6 from '../../public/blog/ts/image6.png';
import image7 from '../../public/blog/ts/image7.png';
import image8 from '../../public/blog/ts/image8.png';
import image9 from '../../public/blog/ts/image9.png';
import image10 from '../../public/blog/ts/image10.png';
import image11 from '../../public/blog/ts/image11.png';
import image12 from '../../public/blog/ts/image12.png';
import image13 from '../../public/blog/ts/image13.png';
import image14 from '../../public/blog/ts/image14.png';

# 构建开源 AI 硬件评测工具， ByteMLperf-v1.0 X 天数智芯，扬长避短是关键

## 1、引言-评估方法论

计算机系统评估与性能分析旨在运用测量、模拟等手段及工具，获取计算机执行任务时的性能特征，识别性能瓶颈，从而达成计算机系统的评估。本博客计划依据此方法论对一款天数智芯 GPGPU 芯片（以下简称“天数芯片”）开展系统评测，通过分阶段测试芯片在指令、算子、模型等多个维度的性能表现，助力读者更深入地了解该芯片的架构、性能表现及使用方法，探寻更为高效的架构与使用方式。

<img src={image1} alt="image1"></img>

本博客将对硬件能力与软件优化这两大方向展开系统化评估。硬件能力层面涵盖单芯片硬件架构（如计算架构、存储层次架构、片内互连架构等）以及多芯片互联架构（包括硬件组件间的通信拓扑、通信接口、通信协议等）。软件优化层面包含图优化、算子优化、指令编译优化等多维度的优化手段。整体评估性能指标包括指令吞吐与延迟、访存吞吐与延迟、算子效率以及集合通信吞吐与延迟等，旨在量化硬件在不同计算和通信任务中的表现。

具体而言，本博客将呈现实际量化评估的流程：首先，从理论和分析层面对比硬件参数（算力、内存容量、互连带宽），并分析通信拓扑；其次，在单芯片层面测试经典指令与算子（如计算密集型、访存密集型、通信密集型等）；最后，在多芯片层面先选取合适的并行策略（如张量并行（TP）、流水线并行（PP）），并在实际模型（LLaMA、DeepSeek R1）上验证吞吐量，以此综合评估硬件性能和优化策略的成效。

## 2、硬件架构分析

### **2.1** **架构参数分析**

天数智芯 GPGPU 芯片属于通用 GPU 架构类型，用于通用计算和 AI 加速。在计算单元个数上，天数芯片的流多处理器数量较 A800 略少，支持的数据类型包括 FP32、FP16、INT8 等，其中软件层面上支持了 FP8 和 FP4。在卡间互联方面上，天数芯片使用 PCIe-Switch 进行卡间互联，可实现多卡级联，在一定程度上满足中等规模的联合工作需求。其互联策略更适合通信相对较轻的任务类型，如模型推理阶段的并行计算或局部计算密集型场景。在缓存与片上存储结构上，天数芯片提供多级缓存体系，包括 L1、L2 和共享内存，但结构细节与 A800 有所不同，可减少一定程度的数据访问延迟，为密集型算子提供较好的局部数据复用能力。

此外，我们对关键指标进行分析，即 FP32 向量与 FP16 矩阵算力比。此指标能够反映一款芯片设计时的权衡取舍，同时也能够解释天数芯片效率高的原因。天数芯片的算力比为 12.2%，而 A800 的算力比为 6.25%。由此可见，天数芯片的比值几乎是 A800 的两倍（这一点对于大模型中 FA 算子的加速大有裨益）。

### 2.2 GPGPU 架构分析与对比

天数芯片和英伟达 A800 80GB SXM 采用相似的 GPGPU 架构，兼容了 GPU Processing Cluster、Texture Processing Cluster、Streaming Multiprocessor、Tensor Core 和 Cuda Core 等概念。计算层次为 GPC-TPC-SM-Cuda/Tensor Core，天数芯片单个芯片包含 4 个 GPU Processing Cluster (GPC)，每个 GPCs 包含 4 个 Texture Processing Cluster (TPC)，每个 TPC 包含 4 个 Streaming Multiprocessor(SM)，每个 SM 都有相应的张量核心/矢量核心。芯片的内存层次由外部显存、片上二级缓存以及一级缓存/共享存储构成，L2 和 L1/shared memory 之间以 Crossbar 的方式连接，每 4 个 SM 共享一个 L1/shared memory 以节省 L2-L1/shared memory 的布线资源。

与英伟达 A800 对比而言， A800 有 8 个 GPCs， 每个 GPCs 有 8 个 TPCs，每个 TPCs 有 2 个 SMs，整个芯片有**128**个 SMs (也有版本 108SMs)。 所以从 SM 的角度而言，**天数芯片约是 A800 的 50%（每个 SM 内计算能力在 FP16 数据精度类型情况下天数芯片是 A800 的 50%）**。

从内存层次架构看， 天数芯片有 64GB 外部显存，16MB L2 cache，192KB L1 cache/shared memory。在 A800 对应的架构中，每个 NV SM 独享一个 192KB L1 存储。**天数的优势是 4 个 SM 共享一个 L1， 进行数据共享，减少对 L2 的带宽需求，进而降低布线需求**。

<div style={{ display: 'flex', gap: '2px', justifyContent: 'center' }}>
  <div style={{ textAlign: 'center' }}>
    <img
      src={image2}
      alt="image2"
    />
    <div style={{
      marginTop: '8px',
      fontSize: '14px',
      color: '#666',
      fontStyle: 'italic'
    }}>
      天数芯片硬件架构
    </div>
  </div>

  <div style={{ textAlign: 'center' }}>
    <img
      src={image3}
      alt="image3"
    />
    <div style={{
      marginTop: '8px',
      fontSize: '14px',
      color: '#666',
      fontStyle: 'italic'
    }}>
      英伟达A800芯片硬件架构 【参考白皮书】
    </div>
  </div>
</div>

### 2.3 单 SM 硬件架构分析与对比

下图左图展示了天数芯片单个流式多处理器（SM）的硬件架构，右图呈现了英伟达 A800 GPU 的 SM 硬件架构。二者的相同点在于，这两款芯片的 SM 中张量核心（兼容 Tensor Core）与矢量核心（兼容 cuda core 包含特殊函数单元，SFU）是分离的，可并行执行任务，且能实现数据共享，这对挖掘数据复用性十分有益。

<div style={{ display: 'flex', gap: '32px', justifyContent: 'center' }}>
  <div style={{ textAlign: 'center' }}>
    <img
      src={image4}
      alt="image4"
    />
    <div style={{
      marginTop: '8px',
      fontSize: '14px',
      color: '#666',
      fontStyle: 'italic'
    }}>
      天数智芯单个SM的硬件架构
    </div>
  </div>

  <div style={{ textAlign: 'center' }}>
    <img
      src={image5}
      alt="image5"
    />
    <div style={{
      marginTop: '8px',
      fontSize: '14px',
      color: '#666',
      fontStyle: 'italic'
    }}>
      英伟达A800单个SM的硬件架构【参考白皮书】
    </div>
  </div>
</div>

两款芯片的差异体现在细微之处。A800 每个 SM 包含 4 个张量核心，每个张量核心每个时钟周期可实现 256 次半精度浮点（FP16）融合乘加（FMA）运算，因此每个 SM 可实现 1024 次稠密 FMA 运算。而天数智芯的一个 SM 配备 8 个张量核心，每个张量核心每个时钟周期可实现 64 次 FP16 FMA 运算，故每个 SM 可实现 512 次稠密 FMA 运算。同时，考虑到无需支持科学计算应用，且专注于大语言模型，天数智芯的芯片去掉了对双精度浮点（FP64）的支持。

在调度层面，天数智芯借助一个调度器和一个标量算术逻辑单元（Scalar ALU），完成了单个 SM 内 4 个核心的计算调度。而 A800 则采用 4 个线程束调度器（Warp Scheduler）来实现单个 SM 内 4 个核心的调度。从这些设计选择来看，A800 的调度方式更为灵活，而天数智芯的芯片则更节省用于调度器的芯片面积。

### 2.4 架构亮点

天数芯片的架构设计具备以下架构优势：

- 采用 GPGPU 架构，且在设计时考虑 CUDA 兼容，因此可以高效支持 CUDA 框架以及相关生态，可快速适配人工智能领域的新模型，在 AI 等领域具有较强的适应性。
- 缓存复用效率较高。通过精妙的硬件分层设计，达成了类似于 H100 上 DSM 的高效共享内存，且访问延迟显著低于 H100。因此，天数芯片能够以更低的功耗和更小的面积提供更大的算力。
- 调度支持度高，支持通信与计算流程的高度自定义，对通信计算融合、算子融合以及不同内核的灵活调度等方面的支持良好，易于通过软件优化实现硬件利用率的最大化。
- 相较于 A800，SME Engine（一种异步 DMA 引擎） 和 Scalar Unit 等设计可在更低能耗下实现更高的计算及访存效率，并且在硬件层面能够高效支持非 k - major 的 int8 格式矩阵运算。

## 3、软件优化分析

### 3.1 类 CUDA 编程生态

天数芯片采用和 CUDA 一致的异步编程模型，在 CUDA/C++语法、计算网络划分（grid，block，warp）以及计算流控制（stream）等方面都提供了高度兼容的开发体验，同时通过优秀的软硬件协同能力，达到对底层硬件资源的细粒度控制，最大化利用每一份计算资源。

以计算流为例：软件上，支持兼容 CUDA stream 形式的 launch -> execute -> wait 异步流程；硬件上，天数芯片有多个流式调度单元，能够根据任务属性，动态分发不同流的计算 block 和通信任务。用户可通过多流的控制实现 SM 效率以及计算/通信能力的最大化。天数芯片也支持兼容 CUDA Graph 等特性，对于需要低延迟、高吞吐量的推理场景，能够大幅提升调度效率。

下图展示了天数 GPU 的软件栈架构，可以兼容绝大多数 cuda 生态，其区别主要是为了达到更高硬件效率而设计的特殊指令，如类似 cp.async 的 shared memory engine 指令，以及高效率的 tensor core 指令等。

<img src={image6} alt="image6"></img>

### 3.2 会用才是关键

虽然 A800 的纸面数据略胜一筹，但实践是检验真理的唯一标准，两者实际性能表现如何，必须放在具体的场景中来对比，而其中**会用**才是关键。在天数芯片上调优 GEMM/FA 等算子的过程中，由于天数拥有从算子库到编译器完整的软件栈，以及硬件 RTL 仿真环境，所以可以在每一处细节上进行极致调优。

例如在进行 FA 算子调优时，先在算法层面做好分块策略以及数据复用之类的工作，再通过分析指令流以评估编译器是否有调优空间，最后通过实测以及硬件仿真得出实际的性能。而仿真波形可以帮助分析硬件 Matrix、Vector 以及 LSU 单元的效率，这个结果可以反馈给算子或编译器，通过继续调优以提高硬件各功能单元的效率，将硬件性能压榨到极致。而在 A800 上调优算子的过程中，由于缺乏硬件 RTL 仿真环境，无法利用仿真波形来指导算子和编译器，所以想要达到如天数芯片这样清楚每一处细节，是完全做不到的。在天数芯片上极致调优 kernel 流程如下图所示，MFU 使用 case QKV BF16, batch=8, head_num=8, seqlen=1024, head_dim=128, causal=false 测得。

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src={image7} alt="image7" />
</div>

| **FA 优化方案**                                                     | **MFU(%)** | **FA 当前方案瓶颈分析**                                                                    |
| ------------------------------------------------------------------- | ---------- | ------------------------------------------------------------------------------------------ |
| Naive 实现                                                          | 13.26      | q_seqlen 维度并未并行，且分块 128 太小                                                     |
| 优化版本 V1（将分块设为 256，且 q_seqlen 维度并行）                 | 26.53      | Gemm QK 使用 single buffer，未隐藏访存延迟（硬件仿真结果反馈）。                           |
| 优化版本 V2（Gemm QK 使用 double buffer，且提前发 Load V 访存指令） | 40.81      | SHFL 指令比较多，且 max 和 sum 需要通过 shared memory swizzle.                             |
| 优化版本 V3 (交换 QK 矩阵位置)                                      | 51.02      | Softmax 计算 max 和计算 sum 指令过多，延迟长（硬件仿真结果反馈，且需要编译器同步做优化）。 |
| 优化版本 V4 （softmax 指令调整，中间计算结果复用并调整指令排布）    | 56.12      | SFU 指令过多（硬件仿真结果反馈）                                                           |

下面具体对大模型中最关键算子 FlashAttention（FA）进行分析。

1. 首先，FA 算子的 softmax 操作会产生大量的 FP32 指令，而天数芯片的向量和矩阵算力比是 A800 的两倍。因此，天数充足的向量算力能够确保 FA 算子在该款芯片上不会受限于向量算力，这有利于提高矩阵单元的利用率。
2. 其次，由于 FA 算子包含大量的 exp 计算，所以 SFU 的算力对于 FA 性能至关重要。天数芯片和 A800 的 SFU 与 FP32 的算力比例相同，均为 1:4，因此天数芯片具备更强大的 SFU 算力。

而以上只是在算子调优过程中体现出来的“**会用**”，除此之外，在进行系统调优时，“**会用**”更加关键。天数芯片采用 PCIE Switch，互联带宽逊色于 A800 SXM。在调优大模型多卡推理时，由于涉及到复杂的通信和计算，如何让通信和计算 overlap 起来，提高计算吞吐的同时隐藏通信的延迟变得尤为关键。而天数做到了通算深度融合，卡间通信完全被计算所掩盖。这块内容后文会详细解释。

### 3.3 指令吞吐

天数芯片的矢量核心计算单元是由 BFU（Basic Function Unit）和 SFU（Special Function Unit）组成，其中 BFU 支持 INT32/FP32 的计算，SFU 为特殊函数计算单元，用于 sin/cos/log/exp/sqrt/rsq/ecp 等计算指令。同时由于天数芯片采用和兼容 CUDA 的异步编程模型，只需要掌握基础 cuda 编程技术即可将矢量核心计算单元的算力发挥到理论峰值的 95%以上。

在 ByteMLPerf 中的 byte_micro_perf 中，我们提供了测试程序和 kernel 程序，测试数据和伪代码如下所示。通过运行测试程序可以测算出在 FP32 数据格式下，BFU 和 SFU 的实际算力为接近 100%的算力利用率。由于 vector 单元的利用率高以及 vector 和 tensor core 单元可以并行执行，在大模型训练推理中，天数这款芯片可以很好的将 SFU 计算隐藏在 Tensor core 计算中，提高计算效率。

| **指令种类(数据类型)** | **利用率** |
| ---------------------- | ---------- |
| EXP(float)             | 97.4%      |
| sin(float)             | 97.4%      |
| cos(float)             | 97.4%      |
| FMA(float)             | 99.7%      |
| MAX(float)             | 99.7%      |
| EXP(half)              | 99.7%      |
| dot2(half)             | 99.4%      |

伪代码如下：

```cpp
//device kernel function
__global__ void test_fma(const float* A，const float* B，float* C，int numElements，int iterCnt)
{
    unsigned ThreadId = computeCurrentThreadId();
    check(ThreadId < numElements);
    float a,b <- A[i],B[i];
    for i from 0 to iterCnt-1 do:
    {
        Sum1 = fma(a，b，Sum1); //Avoid compiler optimization
    }
    C[i] = Sum1;
}

//host main
int main(void)
{
    int  iterCnt = 1 << 16;
    // Allocate the host input vector A B C
    malloc(...);
    // Initialize the host input vectors
    Init(...);
    // Allocate and the device input vector d_A d_B d_C
    cudaMalloc(...);
    // cudaMemcpyHostToDevice
    cudaMemcpy(...);
    // Launch the CUDA Kernel
    int threadsPerBlock = 1024;//Define the number of threads per block
    int blocksPerGrid   = ceil(numElements，threadsPerBlock);//Define the number of blocks per grid
    test_fma<<<blocksPerGrid，threadsPerBlock>>>(d_A，d_B，d_C，numElements，iterCnt);//warm up
    for (int i = 0; i < loop; i++)
        test_fma<<<blocksPerGrid，threadsPerBlock>>>(d_A，d_B，d_C，numElements，iterCnt);
    computeElapsedTime();
    computePerf();
    return 0;
}

```

### 3.4 访存带宽

为了测试天数芯片的实际访存带宽利用率，我们使用了 LLM decode 过程中的典型内存瓶颈算子 GEMV 并考虑了 AWQ 量化方法。具体而言，AWQ 量化的权重是 uint4，激活是 fp16，scale groupsize 是 128。GEMV 是 decode 阶段的算子，即矩阵乘向量，由于序列长度较小，所以一般是内存瓶颈。用 AWQ GEMV 来测试 MBU，实测数据如下：

| **batch** | **n** | **k** | **MBU（%）** |
| --------- | ----- | ----- | ------------ |
| 4         | 8192  | 8192  | 87.8         |
| 8         | 8192  | 8192  | 90.39        |
| 16        | 8192  | 8192  | 91.52        |
| 32        | 8192  | 8192  | 92.19        |

伪代码如下：

```cpp
// int4-bf16 gemv kernel function
__global__ void gemv(void* weight，void* input，void* output) {
    // double buffer shared memory
    __shared__ char buffer[2][block_size];
    // SME(shared memory engine)
    // 2D async global -> shared memory
    load_weight_async(0);
    load_weight_async(1);
    for (int i = 0; i < iter; ++i) {
        // load input vector and scale value
        load_input_scale();
        // wait for async loaded weight
        // allow 1 transfer on the way
        wait_group(1);
        // load next block of weight
        load_weight_async((i + 1) % 2);
        // CTA program counter sync
        cta_barrier();
        // do gemv compute
        compute_gemv();
    }
    // write back to output
    write_back_output();
}

```

得益于先进的异步 SME Engine 设计，较为灵活的架构和较高的 vector 算力，天数芯片在 AWQ GEMV 中能达到超过 90%的带宽利用率。

### 3.5 GEMM 算子实测分析

基于 ByteMLPerf 中几种常用的矩阵规模，对 INT8 GEMM 算子进行了评估，并测量了天数芯片与 A800 在这些 shape 下所能达到的算力利用率。

|      |      |      | 算力利用率（%） |       |
| ---- | ---- | ---- | --------------- | ----- |
| M    | N    | K    | 天数芯片        | A800  |
| 768  | 8192 | 1024 | 97.03           | 50.41 |
| 896  | 8192 | 1024 | 87.64           | 50.26 |
| 1152 | 8192 | 1024 | 93.74           | 51.79 |
| 1408 | 8192 | 1024 | 93.46           | 57.76 |
| 1152 | 1024 | 8192 | 65.9            | 29.30 |
| 1920 | 1024 | 8192 | 97.88           | 48.1  |
| 2688 | 1024 | 8192 | 96.46           | 67.21 |

从结果可以看到，在多种典型规模下，天数芯片能够维持较高的算力利用率，特别是在大尺寸矩阵计算中，其计算阵列能够得到充分发挥，表现出较强的硬件效率稳定性。

### 3.6 FA 算子实测分析

#### 3.6.1 纯 BF16 类型 MFU 对比

我们将针对天数芯片和英伟达 A800 的 FA 算子的纯 BF16 类型 MFU 进行对比。如下表所示，q_head_dim 等于 kv_head_dim，横坐标表示输入序列的长度，纵坐标表示 MFU。

A800 FA 测试使用的是目前最常用的 Dao-AILab 开源 flash-attention 实现，版本为 Release v2.7.4。 天数芯片 FA 测试使用 ByteMLPerf 框架。

| **QKV BF16 的 FA 算子** |            |             |         |          | A800   | 天数芯片 |
| ----------------------- | ---------- | ----------- | ------- | -------- | ------ | -------- |
| batch                   | q_head_num | kv_head_num | seq_len | head_dim | MFU(%) | MFU(%)   |
| 1                       | 64         | 8           | 512     | 128      | 18     | 28.1     |
| 1                       | 64         | 8           | 1024    | 128      | 33     | 47.7     |
| 1                       | 64         | 8           | 2048    | 128      | 45.3   | 61.6     |
| 1                       | 64         | 8           | 4096    | 128      | 63.1   | 70.1     |
| 1                       | 64         | 8           | 8192    | 128      | 68.6   | 75.7     |
| 1                       | 64         | 8           | 16384   | 128      | 69     | 78.6     |

<img src={image8} alt="image8"></img>

如上图所示，从 QKV BF16 的性能测试结果来看，无论在短 seqlen 还是长 seqlen 上，天数芯片相较 A800 都表现出更高的 MFU，这展现出了天数 GPGPU 极高的 FA 效率，充分体现了天数 GPGPU 架构的高效性。

#### 3.6.2 INT8 类型 MFU 对比

下表是天数芯片 QKV Int8 类型的 FA 测试结果。

| **QKV INT 的 FA 算子** |            |             |         |          | **天数芯片** |
| ---------------------- | ---------- | ----------- | ------- | -------- | ------------ |
| batch                  | q_head_num | kv_head_num | seq_len | head_dim | MFU(%)       |
| 1                      | 64         | 8           | 512     | 128      | 20.8         |
| 1                      | 64         | 8           | 1024    | 128      | 35.96        |
| 1                      | 64         | 8           | 2048    | 128      | 47.44        |
| 1                      | 64         | 8           | 4096    | 128      | 56.12        |
| 1                      | 64         | 8           | 8192    | 128      | 61.68        |
| 1                      | 64         | 8           | 16384   | 128      | 64.54        |

#### 3.6.3 FA 流水线分析图

了进一步研究 FA 算子的效率，下面给出天数芯片的**FA** **极致优化的流水图**。如下图所示，可以看出流水线排布非常紧凑，资源空闲非常低，因此其 FA 算子效率较高。

<img src={image9} alt="image9"></img>

## 4、集合通信和通算融合分析

### 4.1 单机拓扑

<img src={image10} alt="image10"></img>

上图为天数芯片的 16 卡互联拓扑图。每两张卡之间通过一个 PCIe 4.0 Switch 连接，每 4 张卡之间通过一个 PCIe 4.0 Switch 连接，两个 PCIe Switch 连接到两个 CPU 上。对于 All-Gather 调用，执行 Ring 算法，经过 15 次环形传输，每个 GPU 上都有了完整的副本。对于 All-Reduce 调用，则使用 Reduce-Scatter + All-Gather 算法，执行两遍 Ring 算法，每一遍算法同样迭代 15 次。

### 4.2 All-Reduce 通信

我们采用 ByteMLPerf 框架测试 2 卡间不同通信量下的通信延迟和算法带宽。天数芯片采用 PCIe 4.0 通信，理论上限为单向 32GB/s，双向 64GB/s。

| **卡数** | **数据类型** | **数据量(MB)** | **latency(us)** | **bus_bw(GB/s)** |
| -------- | ------------ | -------------- | --------------- | ---------------- |
| 2        | float        | 8              | 1310.304        | 12.804           |
| 2        | float        | 16             | 2163.918        | 15.506           |
| 2        | float        | 32             | 3515.623        | 19.089           |

All-Reduce 使用 Ring 算法，对于双卡 Reduce 该算法包含三个核心步骤：

1. 数据传输：将当前的 Rank 的数据发送到下一个 Rank 中；
2. Barrier：在 Rank 发送数据完成后，会在 Rank 间进行一次 Barrier，用于确保数据传输完成；
3. Reduce：将接收的数据和 Rank 上的本地数据进行 Reduce。

再加上存在一定的 PCIe 协议开销和通信启动开销等，在 16M 长度下，all-reduce 能达到约 60%的带宽利用率。

### 4.3 All-Gather 通信

| **卡数** | **数据类型** | **数据量(MB)** | **latency(us)** | **bus_bw(GB/s)** |
| -------- | ------------ | -------------- | --------------- | ---------------- |
| 2        | float        | 8              | 587             | 14.291           |
| 2        | float        | 16             | 1048.4          | 16.003           |
| 2        | float        | 32             | 1770.8          | 18.949           |

All-Gather 的算法与 All-Reduce 类似，区别在于不需要 Reduce。在 16M 长度下，all-gather 同样能达到约 60%的带宽利用率。

### 4.4 通算融合

天数针对大模型（LLM）中常见的 GEMM + All Reduce 流程进行了通算融合优化。得益于高度灵活的兼容 CUDA 架构，用户能够轻松进行计算/通信的细粒度控制，从而提高芯片的利用率。

<div style={{ display: 'flex', gap: '32px', justifyContent: 'center' }}>
  <div style={{ textAlign: 'center' }}>
    <img
      src={image11}
      alt="image11"
    />
  </div>

  <div style={{ textAlign: 'center' }}>
    <img
      src={image12}
      alt="image12"
    />
  </div>
</div>

如上图所示，经过优化，计算和通讯阶段都被拆成了多份，因此不需要等到所有计算完成再开始通讯。经过合理的调度，通讯时间几乎完全隐藏在了 GEMM 和 Reduce 计算过程中，这使得天数芯片能够充分利用有限的互联带宽达到极高的 Prefill 效率。

## 5、模型实测分析

### 5.1 理论分析 （TP v.s. PP）

由于天数芯片的算力密度较高，因此更适用于计算密集型的 **prefill** 场景，而对于访存密集型的 **decode** 场景则相对不占优势。

该芯片采用 PCIe-Switch 进行卡间互联，缺乏专门的高速互联总线，互联能力为 64GB/s 双向带宽，显著低于 A800 的 400GB/s 带宽。因此，在多卡并行时需要选择**带宽需求更低的并行策略**以避免性能瓶颈。尽管互联带宽更低，但就像 3.2 节里面说的那样，软件架构上的优化可以缩小硬件带来的差距，合理的软件设计与调度策略才是性能发挥的关键。

以 Deepseek R1 模型为例，权重使用 int4 存储时，可以在 16 张天数芯片上部署。假设序列长度 4096，模型维度为 7168，每张卡激活参数量 2.3 B，模型有 61 层。当 attn 部分使用 TP，ffn 部分使用 TP/EP 时，每层共需要 2 次 all-reduce 通信操作。当通信数据类型为 BF16 时，在 TP16 时，每张卡的通信量为：

```
seq_len * hidden_size * (TP -1)/TP * 2 * size_bfloat16 * 2 * layer
= 4096 * 7168 * 15/16 * 2 * 2 * 2 * 61
= 12.5GByte
```

而对应的计算量为：

```
seq_len * nparam * 2
= 4096 * 2.3 * 10^9 * 2
= 18.8T
```

虽然实际执行过程中部分算子为 memory-bound（执行时间会比纯计算长），同时可以结合使用通算融合优化，可以把该比例进行提高，通信所占时间仍较大。因此在天数芯片里 TP 使用较少，PP 使用较多。

考虑仅使用 PP 并行时，当 PP=16，每张卡的通信量为：

```
seq_len * dmodel * size_bfloat16
= 4096 * 7168 * 2
= 56MByte
```

可见 PP 的通信开销远小于 TP。

相比于 TP 并行策略，PP 和 CPP 并行策略只有点对点通信，由上述计算可知其通信的理论耗时不到计算时间的 1%，因此目前天数芯片适合低带宽需求的 PP 和 CPP 场景。下图是该芯片在一些常见尺寸下的 P2P 性能。

| **数据类型** | **序列长度(seq_len)** | **模型维度(dmodel)** | **Latency(us)** | **带宽(GB/s)** |
| ------------ | --------------------- | -------------------- | --------------- | -------------- |
| bfloat16     | 4096                  | 8192                 | 3056.36         | 21.957         |
| bfloat16     | 8192                  | 8192                 | 5722.575        | 23.454         |
| bfloat16     | 16384                 | 8192                 | 10994.94        | 24.414         |

在几种序列长度下，P2P 性能能够达到峰值的 65%~75%，能够满足 PP 并行的要求。因此天数芯片主要使用 PP 并行时，可以忽略其通信时间。

目前天数芯片采用 PCIe 4.0 互联方案，若想支持更大 TP 并行，需要将计算/通信时间比进一步提高。因此在后续芯片设计中拟将互联方案升级到 PCIe 5.0 使得带宽翻倍。这样的话在算力不变的情况下，TP 理论计算/通信时间比会同步翻倍。同时由于即使使用 PCIe5.0，纯 TP 理论计算/通信时间仅能从 0.5 提升到 1，计算占比仍较小，所以 PP 仍然需要，所以下一代芯片设计中将更好的支持 P2P 通信语义以更好的支持 PP 和 CPP 并行模式。

下面用 vLLM 框架测试了天数芯片和 A800 在两个经典模型上的 prefill 性能。

### 5.2 LLaMA2 7B Prefill 实测结果

下面测试了英伟达 A800 和天数芯片在 LLaMA2 7B Prefill w8a8-int8 场景下，随着输入 Token 个数增大，TTFT 和吞吐的实测结果

|            |              | **英伟达 A800** |                         | **天数**     |                         |
| ---------- | ------------ | --------------- | ----------------------- | ------------ | ----------------------- |
| **参数量** | **Token 数** | **TTFT(ms)**    | **Throughput(token/s)** | **TTFT(ms)** | **Throughput(token/s)** |
| 7b         | 1024         | 63.55           | 16113                   | 122.15       | 16766                   |
| 7b         | 2048         | 114.65          | 17863                   | 212.21       | 19301                   |
| 7b         | 4096         | 208.40          | 19654                   | 404.12       | 20271                   |

如上表所示，天数芯片的首词生成时间（TTFT）约为英伟达 A800 芯片的 2 倍；随着 Token 数的增加，该芯片吞吐性能相对于 A800 的优势逐步凸显，最终实测吞吐性能略优于英伟达 A800 芯片

### 5.3 DeepSeek R1 Prefill 实测结果

下面测试了英伟达 A800 在 DeepSeek R1 671B Prefill 场景下，随着输入 Token 个数增大，TTFT 和吞吐的实测结果（注：vLLM 在英伟达 A800 芯片上不支持 MoE W8A8-int8，因此 A800 采用 w4a16 量化方案）。为了最大化吞吐量，同时统一测试标准，A800 和天数芯片均采用 TP2 PP8 的并行方案。

| **TP** | **PP** | **参数量** | **Token 数** | **TTFT(ms)** | **Throughput(token/s)** |
| ------ | ------ | ---------- | ------------ | ------------ | ----------------------- |
| 2      | 8      | 671B       | 1024         | 735.67       | 11135                   |
| 2      | 8      | 671B       | 2048         | 1343.85      | 12192                   |
| 2      | 8      | 671B       | 4096         | 2749.54      | 11918                   |

下面是天数芯片在 DeepSeek R1 671B Prefill 场景下，随着输入 Token 个数增大，TTFT 和吞吐的实测结果。由于该芯片的互联带宽较低，因此采用较大的 PP 能够缓解通信的大小，充分发挥芯片的硬件资源，降低延迟，提高吞吐。

| **TP** | **PP** | **参数量** | **Token 数** | **TTFT(ms)** | **Throughput(token/s)** |
| ------ | ------ | ---------- | ------------ | ------------ | ----------------------- |
| 2      | 8      | 671B       | 1024         | 397.93       | 20586                   |
| 2      | 8      | 671B       | 2048         | 770.20       | 21272                   |
| 2      | 8      | 671B       | 4096         | 1627.90      | 20129                   |

通过对比两组实测结果可以发现，得益于天数芯片针对 DeepSeek R1 模型的定制化适配与优化，其 TTFT 显著低于 NVIDIA A800，同时整体吞吐性能也优于 A800。需要指出的是，天数芯片对 DeepSeek 模型进行了 W4A8 量化优化，对应的计算精度为 INT8；而 A800 采用 W4A16 量化方案，对应精度为 BF16。天数芯片更高的吞吐性能主要来源于其硬件算力利用率的提升及深度的软件栈优化。这表明，在 prefill 场景下，天数芯片能够实现与 A800 相近甚至更优的性能表现。

## 6、开源复现

### 6.1 ByteMLPerf 测试

代码版本：

| 测试用例        | 测试配置                                                     | 测试脚本  |
| --------------- | ------------------------------------------------------------ | --------- |
| gemm            | --hardware_type ILUVATAR --device all --task gemm            | launch.py |
| flash attention | --hardware_type ILUVATAR --device all --task flash_attention | launch.py |
| all_reduce      | --hardware_type ILUVATAR --device all --task all_reduce      | launch.py |

代码链接：[https://github.com/sxbjzd/ByteMLPerf/blob/main/byte_micro_perf/backends/ILUVATAR/README.zh_CN.md](https://github.com/sxbjzd/ByteMLPerf/blob/main/byte_micro_perf/backends/ILUVATAR/README.zh_CN.md)

### 6.2 vLLM 测试脚本

代码版本： vllm0.7.3

| 测试用例       | 测试配置                                                                            | 测试脚本             |
| -------------- | ----------------------------------------------------------------------------------- | -------------------- |
| Deepseek w4a8  | --model DeepSeek-R1-int4-pack8 --pipeline-parallel-size 8  --tensor-parallel-size 2 | benchmark_serving.py |
| llama2-7b w8a8 | --model Llama-2-7b-chat-quantized.w8a8 --tensor-parallel-size 2                     | benchmark_serving.py |

代码链接同上。

## 7、讨论与未来展望

### 7.1 算力 v.s. 片上存储

<div style={{ display: 'flex', gap: '32px', justifyContent: 'center' }}>
  <div style={{ textAlign: 'center' }}>
    <img src={image13} alt="image13" />
    <div
      style={{
        marginTop: '8px',
        fontSize: '14px',
        color: '#666',
        fontStyle: 'italic',
      }}
    >
      经典GPU 存储层级
    </div>
  </div>
</div>

在有限的布线资源下如何实现 L2-L1 高带宽以满足快速膨胀的算力的需求是现代 GPGPU 设计的核心挑战。天数芯片的一个架构亮点就是四个 SM 共用一个 L1，从而极大地降低了 L2-L1 的带宽压力，进而降低了布线压力。NVIDIA 在 H 和 B 系列 GPU 里不断通过 DSM 技术来降低 L2 带宽压力。

引申来说这个问题可以进一步扩展为如何设计计算核心-L1 cache-L2 Cache 的分组形式、大小比例、数据通路等架构来尽可能在面积、延迟、带宽、算力、以及算力利用率等诸多因素间达到一个良好的平衡是一个巨大的挑战。

继续引申来看，片上存储与算力配比以及分层形式也是一个均衡的艺术，如何用尽可能少的 buffer 换来尽可能高的算力利用率是所有 AI 加速器芯片架构核心挑战之一。 以 TPU-v1 为例，29%的芯片面积用于片上存储， 仅 24%的面积用于 MAC 计算阵列。 这显然与卖算力的思路相违背。

ByteMLperf 作为芯片架构与实际应用之间的桥梁，通过设计一套完整的性能测试用例，帮助芯片从业者以实际应用的角度探索当前架构的性能瓶颈，为下一代提升芯片架构，探索更优，更更高性能的架构方案提供了相应的解决方案。 在实际应用的角度上， ByteMLperf 提供了多种多样的指令、算子和模型测试， 让芯片在业务上的性能不再是黑盒子，让芯片指标（spec）与实际应用性能重新连接，优化的天花板触手可及。

### 7.2 互联

天数智芯下一代芯片互联方案由包含多颗芯片的多组芯片组成，组内由中间一个 Switch 连接，同时提供 NIC 接口。组内芯片可直连到中间一层的外部 Switch。中间的每个 IX Switch 采用多层堆叠的结构，组内芯片通过组内的 Switch 一跳连接到其他芯片，然后再连接到对应的多个外部 Switch。

多组芯片的总 GPU 芯片数等于芯片组数乘以每组芯片数。同理，中间的 IX Switch 也包括多个 Switch，所以一共的 Switch 个数就等于 IX Switch 组数乘以单组 Switch 个数，实现 Switch 全连接。

### 7.3 未来展望

极致的优化需要更大的视角去看应用、软件、系统架构和芯片。 以下图视角而言，过去 20 年，以 device 优化为主导的摩尔定律，引领了半导体的繁荣。 设计与制程的协同优化 Design-technology-cooptimization (DTCO) 进一步优化了布局布线以及实现了更小的标准单元（standard cell size）。 Chiplet 技术进一步把封装技术带到芯片设计流程以应对更高存储带宽的需求。 每一次优化路径都是更靠近上层应用，需要多层级协同优化。System co-optimization (STCO)的极限端到端优化，更是需要考虑软件和上层应用。 **ByteMLperf**的提出将会成为 STCO 的重工具，通过与芯片厂商的合作探索，实现极限的芯片性能，并为下一代芯片架构设计提供充分的数据支持和优化建议。

<div style={{ display: 'flex', gap: '32px', justifyContent: 'center' }}>
  <div style={{ textAlign: 'center' }}>
    <img src={image14} alt="image14" />
    <div
      style={{
        marginTop: '8px',
        fontSize: '14px',
        color: '#666',
        fontStyle: 'italic',
      }}
    >
      系统协同优化的大视角[图片来源，英特尔75周年的摩尔定律，https://download.intel.com/newsroom/2022/new-technologies/ann-kelleher-iedm-2022.pdf]
    </div>
  </div>
</div>

## 8、结论

ByteMLperf 是一个开源的指令、算子和模型的评测工具，用以探索芯片的天花板，实现买好芯片，用好芯片的目的。 本次博客，以天数智芯 GPGPU 芯片为例进行探索。该款芯片是一款极具特色的芯片，实现和 A800 相当的算力和内存，在实测场景下的 GEMM 算子、FA 算子均能达到较高效率，充分释放了硬件能力。在大模型应用上， 该款芯片通过巧妙的并行策略，扬长避短，实现了很好的性能表现。

与此同时，通过分析和讨论，我们也可以看出天数智芯的架构有很大的迭代空间和发挥潜力，在 ByteMLperf 评测工具的指导和帮助下，如果能利用现有架构优势，发挥长处，补足短板，可以实行更高效的大模型推理性能。同时，天数智芯 SIMT 架构带来的高通用性，灵活性，也为未来大模型结构的变化等做了充足的准备，降低了开发者的学习效率和使用难度。

总之，ByteMLperf 评测工具就是为了和芯片公司协同配合，携手成长，将来也会和芯片公司一起为 AI 基础设施的建立贡献自己的力量。
