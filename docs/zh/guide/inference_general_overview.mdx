import general_framework from '../../public/guide/general_framework.jpeg';

# ByteMLPerf Inference General Perf 概览


ByteMLPerf Inference General Perf是一个AI加速器基准测试工具，专注于从实际生产的角度评估AI加速器，包括软件和硬件的易用性及多功能性。

:::tip 特点：

- 模型和运行环境会更贴近真实业务；
- 对于新硬件，除了评估性能和精度之外，同时也会评估图编译的易用性、覆盖率等指标；
- 在开放Model Zoo上测试所得的性能和精度，会作为新硬件引入评估的参考；

:::

## 整体结构

ByteMLPerf架构如下图所示：

<img src={general_framework} alt="general_framework"></img>

## 模型库列表

ByteMLPerf Inference General Perf支持的模型都收集在模型库(Model Zoo)中。从访问权限的角度看，它们目前分为内部模型和公开模型。与ByteMLPerf Inference General Perf一起发布的是相应版本中包含的公开模型。

公开模型的收集原则：
- 基本模型：包括Resnet50、Bert和WnD；
- 流行模型：包括目前在行业中广泛使用的模型；
- SOTA: 包括与业务领域对应的SOTA模型；

除完整模型结构外，ByteMLPerf Inference General Perf还将添加一些典型的模型子结构子图或OPs（前提是公开模型中找不到包含此类经典子结构的合适模型），例如具有不同序列长度的transformer编码器/解码器，各种常见的卷积操作，如组卷积、深度卷积、点对点卷积，以及常见的rnn结构，如gru/lstm等。

| Model | Domain | Purpose | Framework | Dataset | Precision |
| ---- | ---- | ---- | ---- | ---- | ---- |
| resnet50-v1.5 | cv | regular | tensorflow, pytorch | imagenet2012 | fp32 |
| bert-base | nlp | regular | tensorflow, pytorch | squad-1.1 | fp32 |
| wide&deep | rec | regular | tensorflow | criteo | fp32 |
| videobert | mm  |popular | onnx | cifar100 | fp32 |
| albert | nlp | popular | pytorch | squad-1.1 | fp32 |
| conformer | nlp | popular | onnx | none | fp32 |
| roformer | nlp | popular | tensorflow | cail2019 | fp32 |
| yolov5 | cv | popular | onnx | none | fp32 |
| roberta | nlp | popular | pytorch | squad-1.1 | fp32 |
| deberta | nlp | popular | pytorch | squad-1.1 | fp32 |
| swin-transformer | cv | popular | pytorch | imagenet2012 | fp32 |
| stable diffusion | cv | sota | onnx | none | fp32 |

## 支持的厂商列表
ByteMLPerf Inference General Perf目前支持的厂商后端列表如下所示：

| Vendor |  SKU | Key Parameters | Supplement |
| :---- | :----| :---- | :---- |
| Intel | Xeon | - | - |
| Stream Computing | STC P920 | <li>Computation Power:128 TFLOPS@FP16</li> <li> Last Level Buffer: 8MB, 256GB/s</li> <li>Level 1 Buffer: 1.25MB, 512GB/s</li>   <li> Memory: 16GB, 119.4GB/S</li> <li> Host Interface：PCIe 4, 16x, 32GB/s</li> <li> TDP: 160W </li> | [STC Introduction](https://github.com/bytedance/ByteMLPerf/tree/main/byte_infer_perf/general_perf/backends/STC/README.md) |
| Graphcore | Graphcore® C600 | <li>Compute: 280 TFLOPS@FP16, 560 TFLOPS@FP8</li> <li> In Processor Memory: 900 MB, 52 TB/s </li><li> Host Interface: Dual PCIe Gen4 8-lane interfaces, 32GB/s</li><li> TDP: 185W</li> | [IPU Introduction](https://github.com/bytedance/ByteMLPerf/tree/main/byte_infer_perf/general_perf/backends/IPU/README.md) |
| Moffett-AI | Moffett-AI S30 | <li>Compute: 1440 (32x-Sparse) TFLOPS@BF16, 2880 (32x-Sparse) TOPS@INT8,</li> <li> Memory: 60 GB,</li>  <li> Host Interface: Dual PCIe Gen4 8-lane interfaces, 32GB/s</li> <li> TDP: 250W  </li>                         | [SPU Introduction](https://github.com/bytedance/ByteMLPerf/tree/main/byte_infer_perf/general_perf/backends/SPU/README.md) |

## With ByteIR

ByteIR项目是字节跳动的模型编译解决方案。ByteIR包括编译器、运行时和前端，并提供端到端的模型编译解决方案。 尽管所有的ByteIR组件（编译器/runtime/前端）一起提供端到端的解决方案，并且都在同一个代码库下，但每个组件在技术上都可以独立运行。

更多信息请查看[ByteIR](https://github.com/bytedance/byteir)

ByteIR 编译支持的模型列表:
| Model | Domain | Purpose | Framework | Dataset | Precision |
| ---- | ---- | ---- | ---- | ---- | ---- |
| resnet50-v1.5 | cv | regular | [mhlo](https://lf-bytemlperf.17mh.cn/obj/bytemlperf-zoo/resnet50_mhlo.tar) | imagenet2012 | fp32 |
| bert-base | nlp | regular | [mhlo](https://lf-bytemlperf.17mh.cn/obj/bytemlperf-zoo/bert_mhlo.tar) | squad-1.1 | fp32 |
